# SPDX-License-Identifier: CC0-1.0
# ================= wrap lines at 72 printed characters ================

# You can do whatever. Mirror the whole site, automate downloads,
# whatever. If possible, try to limit the load as reasonable for your
# activities.
User-agent: *
Allow: /

# These user agents should always be allowed to access all files; they
# are likely fetching helpful information for people, downloading raw
# files, archiving the website, or other activities which I am happy to
# facilitate.
User-agent: archive.org_bot
User-agent: ArchiveBot
User-agent: ArchiveTeam
User-agent: Arquivo-web-crawler
User-agent: ChatGPT Agent
User-agent: ChatGPT-User
User-agent: Chrome-Lighthouse
User-agent: curl
User-agent: dosage
User-agent: gabl.ink_responsible_use
Uaer-agent: gallery-dl
User-agent: git
User-agent: ia_archiver
User-agent: ia_archiver-web.archive.org
User-agent: heritrix
User-agent: HTTrack
User-agent: HTTrack Website Copier
User-agent: larbin
User-agent: Meta-ExternalFetcher
User-agent: mirrorweb
User-agent: MSIECrawler
User-agent: Nicecrawler
User-agent: Offline Explorer
User-agent: PerplexityBot
User-agent: SiteSnagger
User-agent: Teleport
User-agent: TeleportPro
User-agent: Validator.nu/LV
User-agent: W3C_CSS_Validator
User-agent: W3C_Validator
User-agent: WebCopier
User-agent: WebZIP
User-agent: Wget
User-agent: wget2
User-agent: YouBot
Allow: /

# As opposed to artificial intelligences (AI) simply accessing the site, these are trainers. I do not consent to my content being used for AI training, regardless of whether your user agent is included below, unless otherwise stated. If you comply with relevant copyright licenses when you output my copyrighted content, you may train your AI on my content. I am fully aware that such a thing is unlikely.
User-agent: Bytespider
User-agent: Claudebot
User-agent: cohere-training-data-crawler
User-agent: FacebookBot
User-agent: Google-Extended
User-agent: GPTBot
User-agent: meta-externalagent
User-agent: PanguBot
Disallow: /
